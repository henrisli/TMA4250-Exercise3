--- 
title: "TMA4250 Spatial Statistics Exercise 3, Spring 2019"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Ã˜yvind Auestad'
header-includes: \usepackage{float}
---


```{r setup, include = FALSE}
library(bookdown)
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE, fig.pos = 'htb')
```


```{r, echo = F, eval = T}
library(reshape2)
library(geoR)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(MASS)
library(cowplot)
library(fields)
library(akima)
library(spatial)
```

# Problem 1: Markov RF

This problem is based on observations of seismic data over a domain $\mathcal{D}\subset \mathbb{R}^2$. The objective is to identify the underlying {sand, shale} lithology distribution over $\mathcal{D}$, represented by $\{0, 1\}$ respectively. 

The observations are collected on a regular $(75 \times 75)$ grid $L_D$, and the seismic data are denoted $\{d(\mathbf{x}); \mathbf{x} \in L_D\};d(\mathbf{x})\in \mathbb{R}$, represented by the $n$-vector $\mathbf{d}$. We retrieve the observations from the R library MASS in the file _seismic.dat_. 

Moreover, observations of the lithology distribution {sand, shale} in a geologically comparable domain $\mathcal{D}_c \subset \mathbb{R}^2$ is available. The lithology distribution is collected on a regular $(66\times 66)$ grid $L_{D_c}$, which has the same spacing as $L_D$ over $\mathcal{D}_c$. Here, we retrieve the observations from the same R library MASS in the file _complit.dat_.

We assume that the underlying lithology distribution can be represented by a Mosaic RF $\{l(\mathbf{x});\mathbf{x}\in L_D\};l(\mathbf{x})\in\{0,1\}=\mathbb{L}$ represented by the $n$-vector $\mathbf{l}$.

## a)
The seismic data collection procedure defines the response likelihood model:
\begin{equation}
[d_i|\mathbf{l}] = \begin{cases} 0.02+U_i, \text{ if } l_i = 0 - \text{ sand},\\ 0.08+U_i, \text{ if } l_i = 1 - \text{ shale}\end{cases}; i = 1,\dots,n,
\label{eq:obs}
\end{equation}
with $U_i, i=1,\dots,n$ i.i.d. $N_1(0,0.06^2)$.


Since the $U_i$'s are all independent, $d_i$ depends only on $l_i$. Hence we get the following likelihood model
\begin{equation}
\begin{split}
[\mathbf{d} | \mathbf{l}] \sim p(\mathbf{d}|\mathbf{l}) &= \prod_{i = 1}^n p(d_i | \mathbf{l}) = \prod_{i = 1}^n p(d_i | l_i) = \prod_{i = 1}^n
\begin{cases}
N(d_i; 0.02, 0.06^2) \ &\textrm{if} \ l_i = 0 \\
N(d_i; 0.08, 0.06^2) \ &\textrm{if} \ l_i = 1
\end{cases} \\
&= \prod_{i : l_i = 0} N(d_i; 0.02, 0.06^2) \prod_{i:l_i = 1} N(d_i; 0.08, 0.06^2).
\end{split}
\label{eq:lik}
\end{equation}

We then display the observations as a map in figure \ref{fig:obs}.
```{r, echo = F, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:obs} Observed values of $\\mathbf{d}$ from the seismic data."}
obs <- read.table("https://www.math.ntnu.no/emner/TMA4250/2017v/Exercise3/seismic.dat")[,1]

color_table = tim.colors(64) 
x = rep(1:75,75)
y = rep(1:75,each = 75)
mtrx3d <- data.frame(x = x, y = y, z = obs)
mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
ggplot(mtrx.melt, aes(x = x, y = y, z = value)) + geom_raster(aes(fill = value)) + scale_fill_gradientn(colours=color_table) + ggtitle("Observed seismic data") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())

#dlist = list(x = 1:75, y = 1:75, z = matrix(obs, nrow = 75, ncol = 75))
#image.plot(dlist, legend.width = 1.9)
```

## b)

We now give $\mathbf{l}$ a uniform constant prior, i.e. $p(\mathbf{l}) = \textrm{const}$. We obtain the following posterior model
$$
[\mathbf{l} | \mathbf{d}] \sim p(\mathbf{l}|\mathbf{d}) = \frac{p(\mathbf{d} | \mathbf{l})}{\sum_{\mathbf{l}' \in L} p(\mathbf{d} | \mathbf{l}')},
$$
where the expression for the likelihood $p(\mathbf{d}|\mathbf{l})$ is given in equation \ref{eq:lik}. This gives
\begin{equation}
\begin{split}
p(\mathbf{l} | \mathbf{d}) &= \frac{ \prod_{i : l_i = 0} N(d_i; 0.02, 0.06^2) \prod_{i:l_i = 1} N(d_i; 0.08, 0.06^2) }{\sum_{\mathbf{k} \in \{0, 1\}^n} \prod_{i : k_i = 0} N(d_i; 0.02, 0.06^2) \prod_{i:k_i = 1} N(d_i; 0.08, 0.06^2) } \\
&= \frac{ \prod_{i : l_i = 0} \exp\big(-\frac{1}{2\cdot 0.06^2}(d_i - 0.02)^2\big) \prod_{i:l_i = 1} \exp\big(-\frac{1}{2\cdot 0.06^2}(d_i - 0.08)^2\big)}{\sum_{\mathbf{k} \in \{0, 1\}^n} \prod_{i : k_i = 0} \exp\big(-\frac{1}{2\cdot 0.06^2}(d_i - 0.02)^2\big) \prod_{i:k_i = 1} \exp\big(-\frac{1}{2\cdot 0.06^2}(d_i - 0.08)^2\big) }\\
&= \frac{ \exp\Big(-\frac{1}{2\cdot 0.06^2}\big(\sum_{i:l_i = 0}(d_i - 0.02)^2 + \sum_{i:l_i = 1} (d_i - 0.08)^2\big) \Big) }{\sum_{\mathbf{k} \in \{0, 1\}^n} \exp\big(-\frac{1}{2\cdot 0.06^2}\big(\sum_{i:k_i = 0}(d_i - 0.02)^2 + \sum_{i:k_i = 1} (d_i - 0.08)^2\big) \big)}
\end{split}
\end{equation}

We want to simulate from the posterior Mosaic RF $\{l(\mathbf{x});\mathbf{x}\in L_D|\mathbf{d}\}$. Because we have an independent prior, each point in the Mosaic RF will be independent, and we get the following posterior distribution
$$[l_i|d_i]\sim p(l_i|d_i) = \frac{p(d_i|l_i)}{\sum_{l_i'\in L} p(d_i|l_i')} = \begin{cases} \frac{N(d_i;0.02,0.06^2)}{N(d_i;0.02,0.06^2)+N(d_i;0.08,0.06^2)} = 1-p_i, &\text{if } l_i = 0,\\ \frac{N(d_i;0.08,0.06^2)}{N(d_i;0.02,0.06^2)+N(d_i;0.08,0.06^2)} = p_i, &\text{if } l_i = 1\end{cases}; i =1,\dots,n.$$
This means that $[l_i|d_i]$ will be Bernoulli distributed with probability $p_i$ as given above. We can simulate from the posterior distribution by simulating each point from a Bernoulli distribution with given $p_i$. Below, we display 10 realizations of the posterior Mosaic RF.
```{r, echo = F, eval = T, out.width = "20%"}
# Algorithm to sample from model with independent prior
# First, calculate all p_i's based on the observations
p = dnorm(obs, 0.08, 0.06^2)/(dnorm(obs, 0.02, 0.06^2) + dnorm(obs, 0.08, 0.06^2))
# Number of points
n = 75*75

plot_image <- function(l){
  mtrx3d <- data.frame(x = x, y = y, z = ifelse(as.vector(l), "Shale", "Sand"))
  mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
  mtrx.melt$value = as.factor(mtrx.melt$value)
  return(ggplot(mtrx.melt, aes(x = x, y = y, fill = value)) +
         geom_raster() + scale_fill_manual("Rock type", values=c("dark blue","dark red")) + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank()))
}


l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)

l = rbinom(n,1,p)
plot_image(l)
```
We see that they are all very similar, and they look reasonable when compared to the observations in figure \ref{fig:obs}. To expand on our analysis, we want to develop expressions for the expectation $E\{\mathbf{l}|\mathbf{d}\}$ and the variances in the diagonal terms of the matrix $\text{Var}\{\mathbf{l}|\mathbf{d}\}$.

We know that each point is independent, and each point conditioned on the observation in that point has a Bernoulli distribution with parameter $p_i$. From theory, we know that if $l_i|d_i \sim \text{Bernoulli}(p_i)$, then $E(l_i|d_i) = p_i$ and $Var(l_i|p_i) = p_i(1-p_i)$. This gives 
$$E\{\mathbf{l}|\mathbf{d}\} = \mathbf{p} = (p_1,\dots,p_n), \quad \text{Var}\{l_i|d_i\} = p_i(1-p_i), i=1,\dots,n.$$

In addition, we want to develop an expression for the maximum marginal posterior predictor $$\text{MMAP}\{\mathbf{l}|\mathbf{d}\} = \hat{\mathbf{l}} = \bigg[\hat{l_i} = \underset{l_i\in\mathbb{L}}{\operatorname{arg max}}\{p(l_i|d_i)\} \bigg]_{i=1,\dots,n}.$$

Because $[l_i|d_i]$ is a Bernoulli distributed variable with parameter $p_i$, we get
$$\hat{l}_i = \begin{cases}0, &\text{if } p_i<0.5,\\ 1, &\text{if } p_i >0.5.\end{cases}$$

Below, we have plotted the expecation, variance and MMAP.
```{r, echo = F, eval = T, out.width = "33%"}
#dlist = list(x = 1:75, y = 1:75, z = matrix(p, nrow = 75, ncol = 75)
mtrx3d <- data.frame(x = x, y = y, z = p)
mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
ggplot(mtrx.melt, aes(x = x, y = y, z = value)) + geom_raster(aes(fill = value)) + scale_fill_gradientn(colours=color_table) + ggtitle("Expected value of l") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
#image.plot(dlist, main = "Expected value of l")

#dlist = list(x = 1:75, y = 1:75, z = matrix(p*(1-p), nrow = 75, ncol = 75))
mtrx3d <- data.frame(x = x, y = y, z = p*(1-p))
mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
ggplot(mtrx.melt, aes(x = x, y = y, z = value)) + geom_raster(aes(fill = value)) + scale_fill_gradientn(colours=color_table) + ggtitle("Variance of l") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
#image.plot(dlist, main = "Variance of l")


plot_image(as.integer(round(p))) + ggtitle("Maximum marginal posterior predictor") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
```

COMMENT ON THE RESULTS
INTERPRET $E(\mathbf{l}|\mathbf{d})$: $E(\mathbf{l}|\mathbf{d})$ is the function of $\mathbf{d}$ which, when predicting $l_i$, attains the minimum squared error. That is, $E(l_i - x)^2$, attains its minimum at $x = E(\mathbf{l_i | \mathbf{d}})$ when $x$ is constrained to be a function of $\mathbf{d}$.

## c)
We then consider a Markov RF prior model for $\{l(\mathbf{x});\mathbf{x}\in L_D\}$, represented by the $n$-vector $\mathbf{l}$, with a neighborhood system $\mathbf{n}_{L}$ consisting of the four closest neighbors of each grid node.

The Markov formulation of the RF is,
$$p(l_i|l_j;j\in \mathbf{n}_i) = const \times \exp\Big(\beta\sum_{j\in \mathbf{n}_i}I(l_i=l_j)\Big);i=1,\dots,n,$$
where $I(\cdot)$ is the indicator function.

As an alternative to the Markov formulation, we can formulate the Markov RF prior model with the Gibbs formulation. To do this, we first specify the clique system. The clique system $\mathbf{c}_L : \{\mathbf{c}_1,\dots,\mathbf{c}_{n_c}\}$ will be a two-node clique system where each clique consists of the two nearest nodes in horizontal or vertical direction. Having defined the cliques, we specify the Gibbs formulation of the Markov RF prior model as,
\begin{equation*}
\begin{split}
\mathbf{l} \sim p(\mathbf{l}) &= const\times\prod_{i=1}^nv_{0l}(l_i)\times\prod_{\mathbf{c}\in \mathbf{c}_L}v_{1\mathbf{c}}(l_j;j\in\mathbf{c})\\ 
&= const \times \prod_{<i,j>}\exp\Big(\beta \sum_{l_i\in\mathbb{L}}I(l_i=l_j)\Big)\\ 
&= const \times \exp\Big(\beta\sum_{<i,j>} \sum_{l_i\in\mathbb{L}}I(l_i=l_j)\Big),
\end{split}
\end{equation*}
where $<i,j>$ represents all nearest neighbors in the grid $L_D$.

We have a response likelihood model, and a Markov RF prior model, which is a conjugate prior model to the response likelihood model. We want to develop expressions for the posterior model on Gibbs and Markov formulation, so we begin by developing the joint distribution of the spatial variable of interest and the observations,
\begin{equation*}
\begin{split}
\begin{bmatrix}\mathbf{l}\\\mathbf{d}\end{bmatrix} \sim &p(\mathbf{l},\mathbf{d}) = p(\mathbf{d}|\mathbf{l})p(\mathbf{l})\\
&= const \times \prod_{i=1}^n p(d_i|l_i) \times \prod_{i=1}^n v_{0l}(l_i) \prod_{\mathbf{c}\in \mathbf{c}_L} v_{1\mathbf{c}}(l_j;j\in\mathbf{c})\\
&= const \times \prod_{i=1,l_i=0}^n N(d_i;0.02,0.06^2) \prod_{i=1,l_i=1}^n N(d_i;0.08,0.06^2) \times \exp\Big(\beta\sum_{<i,j>} \sum_{l_i\in\mathbb{L}}I(l_i=l_j)\Big),
\end{split}
\end{equation*}


\begin{equation*}
\begin{split}
\bigg[\begin{matrix}l_i\\\mathbf{d}\end{matrix}\Big|\mathbf{l}_{-i}\bigg] \sim &p(l_i,\mathbf{d}|\mathbf{l}_{-i}) = p(\mathbf{d}|\mathbf{l})p(l_i|\mathbf{l}_{-i})\\
&= const \times \prod_{i=1,l_i=0}^n N(d_i;0.02,0.06^2) \prod_{i=1,l_i=1}^n N(d_i;0.08,0.06^2) \times \exp\Big(\beta\sum_{j\in \mathbf{n}_i} I(l_i=l_j)\Big), \\
&i = 1,2,\dots,n,
\end{split}
\end{equation*}
where the former is the Gibbs formulation and the latter is the Markov formulation of the joint model. The posterior model for the Markov model formulation is obtained by first dividing by $p(\mathbf{d}_{-i}|\mathbf{l}_{-i})$ and further computing the normalizing constant as $[p(d_i|\mathbf{l}_{-i})]^{-1}$,
\begin{equation*}
\begin{split}
\bigg[l_i\Big|\begin{matrix}\mathbf{d}\\\mathbf{l}_{-i}\end{matrix}\bigg] \sim &p(l_i|\mathbf{d},\mathbf{l}_{-i}) = p(l_i|d_i,l_j,j\in \mathbf{n}_i^l)\\
&= \Bigg[\sum_{l_i'\in \mathbb{L}} p(d_i|l_i')v_{0l}(l_i')w_l(l_i'|l_j;j\in \mathbf{n}_i^l)\Bigg]^{-1}\times p(d_i|l_i) v_{0l}(l_i)w_l(l_i|l_j;j\in \mathbf{n}_i^l)\\
&=\frac{p(d_i|l_i)\times \exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_i=l_j)\Big)}{N(d_i;0.02,0.06^2)\exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_j=0)\Big)+N(d_i;0.08,0.06^2)\exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_j=1)\Big)}\\
&i = 1,2,\dots,n,
\end{split}
\end{equation*}
with $p(d_i|l_i)$ as in equation \ref{eq:obs}.

The corresponding expression for the posterior model on the Gibbs formulation will be,
\begin{equation*}
\begin{split}
[\mathbf{l}|\mathbf{d}] \sim p(\mathbf{l}|\mathbf{d}) = ...
\end{split}
\end{equation*}


To simulate realizations from $\{[l(\mathbf{x}|\mathbf{d}];\mathbf{x}\in L\}$, we need to use an iterative algorithm. The most frequently used algorithm is a McMC/Gibbs algorithm with a single-site proposal pdf based on the Markov formulation of the Markov RF. We implement the following algorithm:
\begin{equation}
\begin{split}
&\text{Define } g(\mathbf{l}'|\mathbf{l}):\\
&i \sim Uni[1,2,\dots,n]\\
&l_i' \sim p(l_i|d_i,l_j;j\in \mathbf{n}_i)\\
&\mathbf{l}' = (l_1,\dots,l_{i-1},l_i',l_{i+1},\dots,l_n)\\
&\text{Initiate:}\\
&\mathbf{l}^0 \text{ such that } p(\mathbf{l}^0|\mathbf{d}) > 0\\
&\text{do for } i=1,2,\dots\\
&\text{Generate: } \mathbf{l}^i \sim g(\mathbf{l}|\mathbf{l}^{i-1})\\
&\text{end do}
\end{split}
\label{eq:alg}
\end{equation}
Asymptotically as $i\rightarrow\infty$ then $\mathbf{l}^i \rightarrow \mathbf{l}^s \sim p(\mathbf{l}|\mathbf{d})$.

A prediction of the spatial variable, represented by the $n$-vector $\hat{\mathbf{l}}$ is usually defined based on a marginal maximum posteriori criterion:
$$\hat{\mathbf{l}} = \bigg[\hat{l}_i = \underset{l_i\in\mathbb{L}}{\operatorname{arg max}}\{p(l_i|\mathbf{d}\}\bigg]_{i=1,\dots,n}.$$
The associated uncertainty quantification is provided by probability spatial variables, $\{p(l(\mathbf{x}) = l|\mathbf{d});\mathbf{x}\in L_D\}$, represented by the $n$-vector $\mathbf{p}_l$ for each $l\in \mathbb{L}$,
$$\mathbf{p}_l = [p_{li} = p(l_i=l|\mathbf{d})]_{i=1,\dots,n}.$$
We then note that the expected value $E(\mathbf{l}|\mathbf{d}) = \mathbf{p}_l$ and the diagonal terms of $\text{Var}(\mathbf{l}|\mathbf{d})$ will be $p_{li}(1-p_{li}), i=1,\dots,n$.

Both the prediction and probability spatial variables must be assessed by sampling based inference, i.e. we generate a set of realizations $\mathbf{l}^s;s=1,2,\dots,n_s$ and assess them by counting estimators.

Now we want to use data from a geologically comparable domain to estimate $\beta$ by a maximum pseudo-likelihood process. First, we display the observations from the domain $D_c$ as a map in figure \ref{fig:complit}.

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:complit} Observations of lithology distribution in a geologically comparable domain to the seismic data."}
complit <- as.matrix(read.table("https://www.math.ntnu.no/emner/TMA4250/2017v/Exercise3/complit.dat"))
complit <- t(complit)
for (i in 1:66){
  complit[i,] = rev(complit[i,])
}

#dlist = list(x = seq(1,50,length.out = 66), y = seq(1,50,length.out = 66), z = complit)
#image.plot(dlist)
x = rep(seq(1,66),66)
y = rep(seq(1,66),each = 66)
#plot_image <- function(l){
#  mtrx3d <- data.frame(x = x, y = y, z = ifelse(as.vector(l), "Shale", "Sand"))
#  mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
#  mtrx.melt$value = as.factor(mtrx.melt$value)
#  return(ggplot(mtrx.melt, aes(x = x, y = y, fill = value)) +
#         geom_tile() + scale_fill_manual("Rock type", values=c("dark blue","dark red")) + coord_fixed(ratio=1))
#}
plot_image(complit)
```
Then, we define the distribution of the vector $\mathbf{l}$, given the model parameter $\beta$,

$$\mathbf{l}\sim p(\mathbf{l};\beta) = const \times \prod_{i=1}^n \exp\Big(\beta \sum_{j\in \mathbf{n}_i}I(l_i=l_j)\Big),$$
which gives the following,
$$[l_i|l_j;j\in \mathbf{n}_i] \sim p(l_i|l_j;j\in\mathbf{n}_i) = \bigg[\sum_{l_i'\in\mathbb{L}}\exp\big(\beta\sum_{j\in\mathbf{n}_i}I(l_i'=l_j)\big)\bigg]^{-1}\times \exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_i=l_j)\Big).$$
Then, we can define the pseudo-likelihood of observing the data, given the parameter $\beta$,
$$\hat{p}(\mathbf{d};\beta) \approx const \times \prod_{i=1}^n \sum_{[l_i';l_j'\in\mathbf{n}_i]\in\mathbb{L}} \prod_{j=i;j\in\mathbf{n}_i} p(d_j|l_j')\times p(l_i'|l_j';j\in\mathbf{n}_i).$$
Because we assume exact observations of the underlying lithology in figure \ref{fig:complit}, we have the following likelihood,
$$[d_i|l_i]\sim p(d_i|l_i) = \delta_{d_i}(l_i),$$
which is inserted into the pseudo-likelihood, together with the distribution of $\mathbf{l}$ to achieve
$$\hat{p}(\mathbf{d};\beta) \propto \prod_{i=1}^n \bigg[\sum_{l_i'\in\mathbb{L}}\exp\big(\beta\sum_{j\in\mathbf{n}_i}I(l_i'=l_j)\big)\bigg]^{-1}\times \exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_i=l_j)\Big).$$
To find the optimal $\beta$, we maximize the logarithm of this pseudo-likelihood, giving
$$\hat{\beta} = \underset{\beta}{\operatorname{arg max}} \{\log(\hat{p}(\mathbf{d};\beta)\} = \underset{\beta}{\operatorname{arg max}} \bigg\{\sum_{i=1}^n\bigg(\beta\big[\sum_{j\in\mathbf{n}_i}I(l_i=l_j)\big]-\log\Big[\sum_{l_i'\in\mathbb{L}}\exp\Big(\beta\sum_{j\in\mathbf{n}_i}I(l_i=l_i)\Big)\Big]\bigg)\bigg\}.$$
```{r, echo = F, eval = T, out.width = "33%"}
complit.vec = as.vector(complit)
neighbors = complit[c(66,1:65),] + complit[c(2:66,1),] + complit[,c(2:66,1)] + complit[,c(66,1:65)]
neighbors.vec = as.vector(neighbors)
equal.neighbors = complit.vec*neighbors.vec+(1-complit.vec)*(4-neighbors.vec)
loglikelihood <- function(beta){
  result = sum(beta*equal.neighbors - log(exp(beta*equal.neighbors) + exp(beta*(4-equal.neighbors))))
  return(-result)
}
optimal = optim(1,loglikelihood, lower = 0)
beta = optimal$par
#plot(x= seq(0,3,length.out=1000), y =sapply(seq(0,3,length.out= 1000),loglikelihood))
#equal.neighbors = complit.vec*neighbors.vec+(1-complit.vec)*(4-neighbors.vec)
#dlist = list(x = seq(1,50,length.out = 66), y = seq(1,50,length.out = 66), z = neighbors)
#image.plot(dlist)
#dlist = list(x = seq(1,50,length.out=66),y=seq(1,50,length.out=66),z=matrix(equal.neighbors,ncol=66))
#image.plot(dlist)
```
We use wrapping boundary conditions on the data displayed in figure \ref{fig:complit} and estimate $\beta$ by the procedure above using the R function `optim()` to find the maximum pseudo-loglikelihood. This gives $\hat{\beta} =$ `r round(beta,2)`.

To assess the posterior model $p(\mathbf{l}|\mathbf{d})$ of the seismic data, we use algorithm \ref{eq:alg} to simulate from the posterior distribution. Because this is an MCMC algorithm, we need some way to justify that our algorithm has converged. To do this, we run three simulations, one starting with $\{l_i=0\}_{i=1,\dots,n}$, one starting with $\{l_i=1\}_{i=1,\dots,n}$ and one looking like a chess board. We then display the proportion of nodes with $l_i=1$ for each iteration in the algorithm. 

```{r, echo = F, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:convergence} Proportion of nodes with $l_i=1$ for three different MCMC chains, one starting with all $l_i=0$, one starting with all $l_i=1$ and one starting as a chess board."}
calculate_neighbors = function(x){
  len = as.integer(sqrt(length(x)))
  mat = matrix(x, nrow = len)
  neighbors = mat[c(len,1:(len-1)),] + mat[c(2:len,1),] + mat[,c(2:len,1)] + mat[,c(len,1:(len-1))]
  return(as.vector(neighbors))
}

simulate_posterior_proportions = function(beta, l_0, iterations){
  l = l_0
  n = length(l_0)
  proportion = rep(NA,iterations)
  for (j in 1:iterations){
    i = sample(1:n,1)
    neighbor = calculate_neighbors(l)[i]
    
    p = dnorm(obs[i],0.08,0.06^2)*exp(beta*neighbor)/(dnorm(obs[i],0.02,0.06^2)*exp(beta*(4-neighbor)) + dnorm(obs[i],0.08,0.06^2)*exp(beta*neighbor))

    l[i] = rbinom(1,1,p)
    proportion[j] = sum(l)/n
    
  }
  return(proportion)
}

simulate_posterior = function(beta, l_0, iterations){
  l = l_0
  n = length(l_0)
  for (j in 1:iterations){
    i = sample(1:n,1)
    neighbor = calculate_neighbors(l)[i]
    
    p = dnorm(obs[i],0.08,0.06^2)*exp(beta*neighbor)/(dnorm(obs[i],0.02,0.06^2)*exp(beta*(4-neighbor)) + dnorm(obs[i],0.08,0.06^2)*exp(beta*neighbor))

    l[i] = rbinom(1,1,p)
  }
  return(l)
}

iter = 80000
sim0 = simulate_posterior_proportions(beta, rep(0,75*75),iter)
sim1 = simulate_posterior_proportions(beta, rep(1,75*75),iter)
sim2 = simulate_posterior_proportions(beta, c(rep(0:1,floor(75*75/2)),1),iter)
df = data.frame(x=1:iter,y1 = sim0, y2 = sim1, y3 = sim2)
ggplot(data = df, aes(x=x)) + geom_line(aes(y = y1, col = "0 initial")) + geom_line(aes(y = y2, col = "1 initial")) + geom_line(aes(y = y3, col = "Chess board")) + theme_bw() + xlab("Iteration") + ylab("Proportion of l_i=1")
#plot(x, sim0$proportion)
#lines(x, sim1$proportion)
#plot(x,sim0$proportion,type="l",col="red")
#plot(x,sim1$proportion,col="green")
#x = rep(seq(1,75),75)
#y = rep(seq(1,75),each = 75)
#plot_image(sim0$l)
#plot_image(sim1$l)
```

From figure \ref{fig:convergence}, we see that after about 60000 iterations, the algorithm seems to converge. Going forward, when simulating from the posterior distribution, we therefore use 60000 iterations in the MCMC chain.

We then display 10 independent realizations as maps below.
```{r, echo = F, eval = T, out.width = "20%"}
mat = matrix(0,ncol = 75*75, nrow = 10)

for (i in 1:10){
  mat[i,] = simulate_posterior(beta, mat[i,], 60000)
  #print(i)
}
x = rep(seq(1,75),75)
y = rep(seq(1,75),each = 75)
plot_image(mat[1,])
plot_image(mat[2,])
plot_image(mat[3,])
plot_image(mat[4,])
plot_image(mat[5,])
plot_image(mat[6,])
plot_image(mat[7,])
plot_image(mat[8,])
plot_image(mat[9,])
plot_image(mat[10,])
```
To the human eye, these look identical to the first model with constant prior.(ADDITIONAL COMMENTS) So, to further investigate our new model, we look at $E(\mathbf{l}|\mathbf{d})$, the diagonal terms of $\text{Var}(\mathbf{l}|\mathbf{d})$ and the prediction $\hat{\mathbf{l}} = \text{MMAP}(\mathbf{l}|\mathbf{d})$ through simulating several independent Markov RFs and then assessing these by counting estimators.

For the expectation and variance we get,
$$E(l_i|d_i) = p_i = \frac{1}{n_s}\sum_{s=1}^{n_s}l_i^s, \quad \text{Var}(l_i|d_i) = p_i(1-p_i).$$
For the $\text{MMAP}(\mathbf{l}|\mathbf{d})$, this will be for each node $l_i$ the mode of all the $n_s$ samples, or
$$\hat{l_i} = \text{Mode}\big(\{l_i^s\}_{s=1,\dots,n_s}\big) = \max_{l_i'\in\mathbb{L}} \sum_{s=1}^{n_s}I(l_i'=l_i^s).$$
We simulate 100 independent realizations with 60000 iterations in each realization and then display the above characteristics as maps.

```{r, echo = F, eval = T, out.width = "33%"}
simulations = 100
iterations = 6000
mat = matrix(0,ncol = 75*75, nrow = simulations)

for (i in 1:simulations){
  mat[i,] = simulate_posterior(beta, mat[i,], iterations)
}
Expected = apply(mat,2,mean)
Var = Expected*(1-Expected)
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
MMAP = apply(mat,2,Mode)


mtrx3d <- data.frame(x = x, y = y, z = Expected)
mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
ggplot(mtrx.melt, aes(x = x, y = y, z = value)) + geom_raster(aes(fill = value)) + scale_fill_gradientn(colours=color_table) + ggtitle("Expected value of l") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())


mtrx3d <- data.frame(x = x, y = y, z = Var)
mtrx.melt <- melt(mtrx3d, id.vars = c("x","y"), measure.vars = "z")
ggplot(mtrx.melt, aes(x = x, y = y, z = value)) + geom_raster(aes(fill = value)) + scale_fill_gradientn(colours=color_table) + ggtitle("Variance of l binomial") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())

plot_image(MMAP) + ggtitle("Maximum marginal posterior predictor") + coord_fixed(ratio=1) +  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
```
MAKE SOME COMMENTS ABOUT THE PLOTS

## d)
MAKE SOME COMMENTS TO COMPARE THE MODELS
